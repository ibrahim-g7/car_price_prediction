{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing neccassary packages \n",
    "import marimo as mo\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting function\n",
    "def splitting(X, y):\n",
    "    # Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.10, \n",
    "        random_state=42\n",
    "    )\n",
    "    # Calidation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.225,\n",
    "        random_state=42\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val, X_test, y_test\n",
    "\n",
    "# Frequency encoder for high cardinality features \n",
    "def frequency_encoder(data_frame, column):\n",
    "    # make a copy\n",
    "    data_frame = data_frame.copy()\n",
    "    # create map\n",
    "    frequency_map = data_frame[column].value_counts(normalize=True)\n",
    "    # make new freq columns\n",
    "    data_frame[column + \"_freq\"] = data_frame[column].map(frequency_map)\n",
    "    data_frame.drop(columns=column, axis=1, inplace=True)\n",
    "    return data_frame\n",
    "\n",
    "# Scaling/Encdoing function \n",
    "def processing(train, val):\n",
    "    # Define scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Make a copy\n",
    "    X_train = train.copy()\n",
    "    X_val = val.copy()\n",
    "    # Define numerical columns\n",
    "    numerical_to_scale = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.to_list()\n",
    "    # Fit and transform on the training split\n",
    "    if len(numerical_to_scale) > 0:\n",
    "        X_train[numerical_to_scale] = scaler.fit_transform(X_train[numerical_to_scale])\n",
    "        X_val[numerical_to_scale] = scaler.transform(X_val[numerical_to_scale])\n",
    "\n",
    "    # OneHot Encoder\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    # Define categorical columns and check cardinality \n",
    "    categorical_to_scale = []\n",
    "    high_cardinality_to_scale = []\n",
    "    # Check if there is any\n",
    "    if len(X_train.select_dtypes(include=\"object\").columns.to_list()) > 0:\n",
    "        for col in X_train.select_dtypes(include=\"object\").columns.to_list():\n",
    "            if X_train[col].nunique() > 50: \n",
    "                high_cardinality_to_scale.append(col)\n",
    "            else:\n",
    "                categorical_to_scale.append(col)\n",
    "        # Check if there is any \n",
    "        if len(categorical_to_scale) > 0: \n",
    "            cat_train = onehot_encoder.fit_transform(X_train[categorical_to_scale])\n",
    "            # Create dataframe of encoded training \n",
    "            cat_cols = onehot_encoder.get_feature_names_out(categorical_to_scale)\n",
    "            cat_train_df = pd.DataFrame(cat_train, columns=cat_cols, index=X_train.index)\n",
    "            # Create dataframf of encoded validation \n",
    "            cat_val = onehot_encoder.transform(X_val[categorical_to_scale])\n",
    "            cat_val_df = pd.DataFrame(cat_val, columns=cat_cols, index=X_val.index)\n",
    "            # Drop original categorical columns and join the encoded ones\n",
    "            X_train = X_train.drop(columns=categorical_to_scale).join(cat_train_df)\n",
    "            X_val = X_val.drop(columns=categorical_to_scale).join(cat_val_df)\n",
    "\n",
    "        # frequency encoding \n",
    "        if len(high_cardinality_to_scale) > 0:\n",
    "            for col in high_cardinality_to_scale:\n",
    "                X_train = frequency_encoder(X_train, col)\n",
    "                X_val = frequency_encoder(X_val, col)\n",
    "\n",
    "    # Define ordinal features\n",
    "    ordinal_to_scale = X_train.select_dtypes(include=\"category\").columns.to_list()\n",
    "    # Check if there is any \n",
    "    if len(ordinal_to_scale) > 0:\n",
    "        # Ordinal Encoding\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        # Define ordinal columns \n",
    "        # Fit and transform on the training .\n",
    "        X_train[ordinal_to_scale] = ordinal_encoder.fit_transform(X_train[ordinal_to_scale])\n",
    "        # Transform the validation \n",
    "        X_val[ordinal_to_scale] = ordinal_encoder.transform(X_val[ordinal_to_scale])\n",
    "\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "# KFold cross validation\n",
    "def k_fold(X_train, X_val, y_train, y_val, alpha, low_verbose=False):\n",
    "    # Seperate columns names by type\n",
    "    categorical_to_scale = X_train.select_dtypes(include=\"object\").columns.to_list()\n",
    "    numerical_to_scale = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.to_list()\n",
    "    ordinal_to_scale = X_train.select_dtypes(include=\"category\").columns.to_list()\n",
    "\n",
    "    # Define variable before KFold CV\n",
    "    ols_scores = []\n",
    "    ridge_scores = []\n",
    "    lasso_scores = []\n",
    "\n",
    "    # 10-fold split as requested in the assignment\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        # Fold split\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index].copy(), X_train.iloc[val_index].copy()\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        X_train_fold, X_val_fold = processing(X_train_fold, X_val_fold)\n",
    "        # Modeling \n",
    "        models = {\n",
    "            \"ols\": LinearRegression(),\n",
    "            \"ridge\": Ridge(alpha=alpha, random_state=42),\n",
    "            \"lasso\": Lasso(alpha=alpha, random_state=42)\n",
    "        }\n",
    "        y_pred = []\n",
    "        for model in models.values():\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "        # Evluating metrics MAE, amd RMSE on both training and validation to detect over/under fitting: \n",
    "        # OLS\n",
    "        ols_pred = models[\"ols\"].predict(X_val_fold)\n",
    "        old_pred_train = models[\"ols\"].predict(X_train_fold)\n",
    "        ols_mae = mean_absolute_error(y_val_fold, ols_pred)\n",
    "        ols_mae_train = mean_absolute_error(y_train_fold, old_pred_train)\n",
    "        ols_rmse = np.sqrt(mean_squared_error(y_val_fold, ols_pred))\n",
    "        ols_rmse_train = np.sqrt(mean_squared_error(y_train_fold, old_pred_train))\n",
    "        ols_r2 = r2_score(y_val_fold, ols_pred)\n",
    "        ols_r2_train = r2_score(y_train_fold, old_pred_train)\n",
    "        ols_scores.append((ols_mae,ols_mae_train, ols_rmse, ols_rmse_train, ols_r2, ols_r2_train))\n",
    "        # Ridge\n",
    "        ridge_pred = models[\"ridge\"].predict(X_val_fold)\n",
    "        ridge_pred_train = models[\"ridge\"].predict(X_train_fold)\n",
    "        ridge_mae = mean_absolute_error(y_val_fold, ridge_pred)\n",
    "        ridge_mae_train = mean_absolute_error(y_train_fold, ridge_pred_train)\n",
    "        ridge_rmse = np.sqrt(mean_squared_error(y_val_fold, ridge_pred))\n",
    "        ridge_rmse_train = np.sqrt(mean_squared_error(y_train_fold, ridge_pred_train))\n",
    "        ridge_r2 = r2_score(y_val_fold, ridge_pred)\n",
    "        ridge_r2_train = r2_score(y_train_fold, ridge_pred_train)\n",
    "        ridge_scores.append((ridge_mae,ridge_mae_train, ridge_rmse, ridge_rmse_train, ridge_r2, ridge_r2_train))\n",
    "        # Lasso \n",
    "        lasso_pred = models[\"lasso\"].predict(X_val_fold)\n",
    "        lasso_pred_train = models[\"lasso\"].predict(X_train_fold)\n",
    "        lasso_mae = mean_absolute_error(y_val_fold, lasso_pred)\n",
    "        lasso_mae_train = mean_absolute_error(y_train_fold, lasso_pred_train)\n",
    "        lasso_rmse = np.sqrt(mean_squared_error(y_val_fold, lasso_pred))\n",
    "        lasso_rmse_train = np.sqrt(mean_squared_error(y_train_fold, lasso_pred_train))\n",
    "        lasso_r2 = r2_score(y_val_fold, lasso_pred)\n",
    "        lasso_r2_train = r2_score(y_train_fold, lasso_pred_train)\n",
    "        lasso_scores.append((lasso_mae,lasso_mae_train, lasso_rmse, lasso_rmse_train, lasso_r2, lasso_r2_train))\n",
    "\n",
    "\n",
    "    # After the loop, print with high or low verbose\n",
    "    if not low_verbose:\n",
    "        print(\"\\nAverage performance across all folds:\")\n",
    "\n",
    "        print(f\"OLS[Val] - MAE: {np.mean([s[0] for s in ols_scores]):.4f}, RMSE: {np.mean([s[2] for s in ols_scores]):.4f}, R2: {np.mean([s[4] for s in ols_scores]):.4f}\")\n",
    "        print(f\"OLS[Train] - MAE: {np.mean([s[1] for s in ols_scores]):.4f}, RMSE: {np.mean([s[3] for s in ols_scores]):.4f}, R2: {np.mean([s[5] for s in ols_scores]):.4f}\")\n",
    "        print(f\"OLS[Train/Val] - MAE: {np.mean([s[1] for s in ols_scores])/np.mean([s[0] for s in ols_scores]):.2f}, RMSE:  {np.mean([s[3] for s in ols_scores])/np.mean([s[2] for s in ols_scores]):.2f}\")\n",
    "\n",
    "        print(f\"Ridge[Val] - MAE: {np.mean([s[0] for s in ridge_scores]):.4f}, RMSE: {np.mean([s[2] for s in ridge_scores]):.4f}, R2: {np.mean([s[4] for s in ridge_scores]):.4f}\")\n",
    "        print(f\"Ridge[Train] - MAE: {np.mean([s[1] for s in ridge_scores]):.4f}, RMSE: {np.mean([s[3] for s in ridge_scores]):.4f}, R2: {np.mean([s[5] for s in ridge_scores]):.4f}\")\n",
    "        print(f\"Ridge[Train/Val] - MAE: {np.mean([s[1] for s in ridge_scores])/np.mean([s[0] for s in ridge_scores]):.2f}, RMSE:  {np.mean([s[3] for s in ridge_scores])/np.mean([s[2] for s in ridge_scores]):.2f}\")\n",
    "\n",
    "        print(f\"Lasso[Val] - MAE: {np.mean([s[0] for s in lasso_scores]):.4f}, RMSE: {np.mean([s[2] for s in lasso_scores]):.4f}, R2: {np.mean([s[4] for s in lasso_scores]):.4f}\")\n",
    "        print(f\"Lasso[Train] - MAE: {np.mean([s[1] for s in lasso_scores]):.4f}, RMSE: {np.mean([s[3] for s in lasso_scores]):.4f}, R2: {np.mean([s[5] for s in lasso_scores]):.4f}\")\n",
    "        print(f\"Lasso[Train/Val] - MAE: {np.mean([s[1] for s in lasso_scores])/np.mean([s[0] for s in lasso_scores]):.2f}, RMSE:  {np.mean([s[3] for s in lasso_scores])/np.mean([s[2] for s in lasso_scores]):.2f}\")\n",
    "    else:\n",
    "        print(f\"Ridge[Train/Val] - MAE: {np.mean([s[1] for s in ridge_scores])/np.mean([s[0] for s in ridge_scores]):.2f}, RMSE:  {np.mean([s[3] for s in ridge_scores])/np.mean([s[2] for s in ridge_scores]):.2f}, R2: {np.mean([s[4] for s in ridge_scores]):.2f}\")\n",
    "        print(f\"Lasso[Train/Val] - MAE: {np.mean([s[1] for s in lasso_scores])/np.mean([s[0] for s in lasso_scores]):.2f}, RMSE:  {np.mean([s[3] for s in lasso_scores])/np.mean([s[2] for s in lasso_scores]):.2f}, R2: {np.mean([s[4] for s in lasso_scores]):.2f}\")\n",
    "        print(\"|:--------------------------------------------------------:|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "Geely Auto, a Chinese car manufacturer wants to enter the U.S.  market by initiating a local manufacturer to compete with both Amerrican and European automakers. To ensure strong an succeful entry to the automobile market, Geely Auto utilized an automobile consulting firm to analyze the key factors influencing car pricing in the American market, which may differ from the Chinese market.\n",
    "\n",
    "# 1. Problem Statement:\n",
    "\n",
    "Geely Auto seek want to know two things:\n",
    "\n",
    "1. Which feature contribute the most in the prediction of car prices. (lasso?)\n",
    "2. How well do these variable explain car pricing trends (R2?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset \n",
    "path = \"../data/CarPrice_Assignment.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "## Data Dictionary\n",
    "- Using the provided datasets from kaggle, we notice the data contrain 26 features with following types\n",
    "\n",
    "|type|count|\n",
    "|:-:|:---:|\n",
    "|float64|8|\n",
    "|int64|8|\n",
    "|String (Objects)|10|\n",
    "\n",
    "The description each features are:\n",
    "\n",
    "| #  | Column Name           | Description                                              | Data Type   |\n",
    "|:----:|:-----------------------:|:----------------------------------------------------------:|:-------------:|\n",
    "| 1  | Car_ID               | Unique id of each observation                            | Integer     |\n",
    "| 2  | Symboling            | Insurance risk rating (+3 = risky, -3 = safe)            | Categorical |\n",
    "| 3  | carCompany           | Name of car company                                      | Categorical |\n",
    "| 4  | fueltype             | Car fuel type (e.g., gas or diesel)                      | Categorical |\n",
    "| 5  | aspiration           | Aspiration used in a car                                 | Categorical |\n",
    "| 6  | doornumber           | Number of doors in a car                                 | Categorical |\n",
    "| 7  | carbody              | Body of car                                              | Categorical |\n",
    "| 8  | drivewheel           | Type of drive wheel                                      | Categorical |\n",
    "| 9  | enginelocation       | Location of car engine                                   | Categorical |\n",
    "| 10 | wheelbase            | Wheelbase of car                                         | Numeric     |\n",
    "| 11 | carlength            | Length of car                                            | Numeric     |\n",
    "| 12 | carwidth             | Width of car                                             | Numeric     |\n",
    "| 13 | carheight            | Height of car                                            | Numeric     |\n",
    "| 14 | curbweight           | Weight of a car without occupants or baggage             | Numeric     |\n",
    "| 15 | enginetype           | Type of engine                                           | Categorical |\n",
    "| 16 | cylindernumber       | Number of cylinders in the car                           | Categorical |\n",
    "| 17 | enginesize           | Size of car engine                                       | Numeric     |\n",
    "| 18 | fuelsystem           | Fuel system of car                                       | Categorical |\n",
    "| 19 | boreratio            | Bore ratio of car                                        | Numeric     |\n",
    "| 20 | stroke               | Stroke or volume inside the engine                       | Numeric     |\n",
    "| 21 | compressionratio     | Compression ratio of car                                 | Numeric     |\n",
    "| 22 | horsepower           | Horsepower of car                                        | Numeric     |\n",
    "| 23 | peakrpm              | Car peak RPM                                             | Numeric     |\n",
    "| 24 | citympg              | Mileage in city                                          | Numeric     |\n",
    "| 25 | highwaympg           | Mileage on highway                                       | Numeric     |\n",
    "| 26 | price (Dependent var)| Price of car                                             | Numeric     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Data Cleaning\n",
    "We note the following:\n",
    "\n",
    "1. The dataset contian 26 feautres and 205 observations (rows) no missing values.\n",
    "2. Since IDs are not usde in mathematical operation, it would be wise to change `car_ID` from integer into a strings/object.\n",
    "3. The `symboling` feature is considered an `int` while it shold be an `categorical` (i.e. ordinal from -3 to +3) as defined in the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value visualization \n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "# Cast car_id into an object\n",
    "df[\"car_ID\"] = df[\"car_ID\"].astype('object')\n",
    "# Cast symboling onto a categorical (ordinal) feature\n",
    "symboling_order = sorted(df[\"symboling\"].unique())\n",
    "df[\"symboling\"] = pd.Categorical(\n",
    "    df[\"symboling\"], categories=symboling_order, ordered=True\n",
    ")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate car name into two columns brand, and fix duplicated names \n",
    "df.insert(0, \"brand\", df[\"CarName\"].str.split(\" \").str[0])\n",
    "df.insert(0, \"model\", df[\"CarName\"].str.split(\" \").str[1:].str.join(\" \"))\n",
    "print(df['brand'].unique())\n",
    "replace_brand = {\n",
    "    \"maxda\": \"mazda\",\n",
    "    \"Nissan\": \"nissan\",\n",
    "    \"toyouta\": \"toyota\",\n",
    "    \"vokswagen\":\"volkswagen\",\n",
    "    \"vw\":\"volkswagen\"\n",
    "}\n",
    "\n",
    "df[\"brand\"] = df[\"brand\"].replace(replace_brand)\n",
    "print(df['brand'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "## Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the cardinality of categorical variable \n",
    "categorical_features = df.select_dtypes(include=[\"category\", \"object\"]).columns.to_list()\n",
    "fig1, ax1 = plt.subplots(math.ceil(len(categorical_features)/3),3)\n",
    "fig1.set_size_inches(15,15)\n",
    "fig1.suptitle(\"Bar plots of categorical features\")\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "for _idx, _feat in enumerate(categorical_features):\n",
    "    _counts = df[_feat].value_counts()\n",
    "    ax1.flatten()[_idx].bar(_counts.index, _counts.values)\n",
    "    ax1.flatten()[_idx].legend\n",
    "    ax1.flatten()[_idx].set(\n",
    "        xlabel=_feat,\n",
    "        ylabel=\"Count\", \n",
    "    )\n",
    "\n",
    "for _idx in range(len(categorical_features), math.ceil(len(categorical_features)/3) * 3):\n",
    "    fig1.delaxes(ax1.flatten()[_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cardinality of categorical variables \n",
    "print(f\"|----------------------------|\")\n",
    "for _feat in categorical_features:\n",
    "    print (f\"The column {_feat} has {df[_feat].nunique()} unique entries\")\n",
    "print(f\"|----------------------------|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "From the above analysis we note the following.\n",
    "\n",
    "1. Most car are of medium risk and leaning toward high risk.\n",
    "2. Most cars use gas more than diseal as fuel.\n",
    "3. More care are naturally aspirated than turbo.\n",
    "4. The majoraty of the cars uses overhead camshaft engine (`ohc` engine) and located at the front.\n",
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Numerical features\n",
    "\n",
    " - Describtive statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describtive statistics of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical features name\n",
    "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.to_list()\n",
    "print(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms of numerical features\n",
    "\n",
    "fig2, ax2 = plt.subplots(math.ceil(len(numerical_features)/3), 3)\n",
    "fig2.suptitle(\"Scatter plots of numerical features\")\n",
    "fig2.set_size_inches(15,15)\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "for _idx, _feat in enumerate(numerical_features):\n",
    "    ax2.flatten()[_idx].hist(df[_feat], bins=int(df.shape[0]**0.5))\n",
    "    ax2.flatten()[_idx].set(\n",
    "        xlabel=f\"{_feat}\",\n",
    "        ylabel=f\"\"\n",
    "    )\n",
    "# Removing extra graphs\n",
    "for _idx in range(len(numerical_features), math.ceil(len(numerical_features)/3) * 3):\n",
    "    fig2.delaxes(ax2.flatten()[_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finidng the IQR \n",
    "q75 = np.percentile(df[\"price\"], 75)\n",
    "q25 = np.percentile(df[\"price\"], 25)\n",
    "iqr = q75 - q25\n",
    "print(df[df[\"price\"] > q75 + 1.5*iqr].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "### From the above analysis we note:\n",
    "\n",
    "1. Most features looks normally distributed except:\n",
    "       1. `carwidth`, `enginesize`, `horsepower`, __price__ is postively skewed\n",
    "       2. `compressionration` has a huge gap between 10 and 20.\n",
    "2. Using interquartile method we notice there is 15 outliers in price. For now we will leave these outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {},
   "source": [
    "# 4. Setup\n",
    "\n",
    "1. Dropping `car_ID`, and 'CarName'.\n",
    "2. Feature/Target splitting.\n",
    "3. Using holdout + CV method for splitting (70% train, 20% validation, 10% testing)\n",
    "4. Using standard scalar for numerical features.\n",
    "5. Using one hot encoder for categorical features.\n",
    "6. Using Ordinal encoder for ordinal features.\n",
    "\n",
    "# 5. Modeling\n",
    "\n",
    "1. Training using OLS, ridge, and lasso.\n",
    "2. Calculate the metrics and save them for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/Target split and dropping car_ID, CarName(replaced with brand, and model)\n",
    "X, y = df.drop(columns=[\"car_ID\", \"CarName\",\"price\"], axis=1).copy(), df[\"price\"].copy()\n",
    "# train/val/test splitting\n",
    "X_train, X_val, y_train, y_val, X_test, y_test = splitting(X, y)\n",
    "# K-Fold corss validation\n",
    "k_fold(X_train, X_val, y_train, y_val, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {},
   "source": [
    "# 6. Model evaluation\n",
    "\n",
    "After training the model we note the following:\n",
    "\n",
    "1. OLS had the highest MAE, and RMSE, while Ridge has the lowest across all folds.\n",
    "2. All the models  are overfitting even at high alpha value ($\\alpha = 0.9$), and ridge being the least in severity. To solve this problem, we will use lasso's feature selection for further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using lasso \n",
    "lasso = Lasso(alpha=0.95)\n",
    "sfm = SelectFromModel(lasso, prefit=False)\n",
    "\n",
    "# Numerical Scaler \n",
    "_scaler = StandardScaler()\n",
    "# Fit and transform on the training  \n",
    "_X_train = X_train.copy()\n",
    "_X_val = X_val.copy()\n",
    "\n",
    "# Proccessing the data \n",
    "_X_train, _X_val = processing(_X_train, _X_val)\n",
    "\n",
    "sfm.fit(_X_train, y_train)\n",
    "X_transformed = sfm.transform(_X_train)\n",
    "\n",
    "selected_features = _X_train.columns[sfm.get_support()].tolist()\n",
    "print(\"Selected features:\", len(selected_features))\n",
    "print(\"Total number of columns after encoding\", len(_X_train.columns.to_list()))\n",
    "\n",
    "# Get feature importances (coefficients) from the Lasso model\n",
    "feature_importances = pd.Series(\n",
    "    sfm.estimator_.coef_, index=_X_train.columns\n",
    ")  # Access coefficients from the estimator_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_importances = feature_importances.abs().sort_values(ascending=False)  # Use absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {},
   "source": [
    "# Repeat: 5. Modeling\n",
    "\n",
    "- Since we obtained the most important features we can now simplify the model by reducing the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a datafrma from the selected features\n",
    "feature_num = len(selected_features)\n",
    "important_features = pd.DataFrame(sorted_importances)\n",
    "important_features.insert(0, \"features\",important_features.index)\n",
    "important_features.rename(columns={0:\"value\"}, inplace=True)\n",
    "important_features.reset_index(drop=True, inplace=True)\n",
    "print(important_features[\"features\"].unique().tolist()[:50])\n",
    "\n",
    "important_features[\"original_features\"] = important_features.apply(lambda row: row[\"features\"].split(\"_\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping to reverse the onehot encoding\n",
    "important_features.head(71)[\"original_features\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of contributing based on the original features\n",
    "important_features.head(71).groupby(\"original_features\")[\"value\"].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "From the results above we note:\n",
    "\n",
    "1.  car brand  contribute the most to the model following by engine type.\n",
    "2.  Based on the above table, I will only considere the highest contributing 12 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start hyperparameter tuning, building an initial model and a new split based on the selected features\n",
    "sig_features = list(important_features.head(feature_num).groupby(\"original_features\")[\"value\"].sum().sort_values(ascending=False).index[0:13])\n",
    "print(sig_features)\n",
    "X_sig = df[sig_features].copy()\n",
    "\n",
    "X_sig_train, X_sig_val, y_sig_train, y_sig_val, X_sig_test, y_sig_test = splitting(X_sig, y)\n",
    "k_fold(X_sig_train, X_sig_val, y_sig_train, y_sig_val, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.0001,0.001, 0.01, 0.5, 1, 5, 10, 15, 20, 35, 50, 75,80,85,90,95, 100, 125,150,175,200,225, 250, 500]\n",
    "for alpha in alpha_values:\n",
    "    print(f\"alpha = {alpha}\")\n",
    "    k_fold(X_sig_train, X_sig_val, y_sig_train, y_sig_val, alpha=alpha, low_verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {},
   "source": [
    "Based on the above analysis, I choose ridge to be my model with hyper parameter $\\alpha = 15$ due to the balance betweet $R^2$ and the degree of bias.\n",
    "\n",
    "---\n",
    "- After doing cross validation, feature selection, and hyperparameter tunning. I will train the final model with 0.8/0.2 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the significant features and split features from target\n",
    "X_final, y_final = df.drop(columns=[\"car_ID\", \"CarName\",\"price\"], axis=1)[sig_features].copy(), df[\"price\"].copy()\n",
    "# Train/test split\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "# Scaling and encoding the trainig set\n",
    "X_train_final, X_test_final = processing(X_train_final, X_test_final)\n",
    "# Defnie the model\n",
    "final_model = Ridge(alpha=15, random_state=42)\n",
    "# Tring the model\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "# Predict the test \n",
    "y_pred_final_train = final_model.predict(X_train_final)\n",
    "y_pred_final_test = final_model.predict(X_test_final)\n",
    "\n",
    "# Calculate metrics:\n",
    "mae_final_train = mean_absolute_error(y_train_final, y_pred_final_train)\n",
    "mae_final_test = mean_absolute_error(y_test_final, y_pred_final_test)\n",
    "\n",
    "rmse_final_train = np.sqrt(mean_squared_error(y_train_final, y_pred_final_train))\n",
    "rmse_final_test = np.sqrt(mean_squared_error(y_test_final, y_pred_final_test))\n",
    "\n",
    "r2_final_train = r2_score(y_train_final, y_pred_final_train)\n",
    "r2_final_test = r2_score(y_test_final, y_pred_final_test)\n",
    "\n",
    "print(f\"Evluation metrics:\")\n",
    "print(f\"Train: MAE = {mae_final_train:.2f}, Test: MAE = {mae_final_test:.2f}\")\n",
    "print(\"|:-------------------------------------:|\")\n",
    "print(f\"Train: RMSE = {rmse_final_train:.2f}, Test: RMSE = {rmse_final_test:.2f}\")\n",
    "print(\"|:-------------------------------------:|\")\n",
    "print(f\"Train: R2 = {r2_final_train:.2f}, Test: R2 = {r2_final_test:.2f}\")\n",
    "print(\"|:-------------------------------------:|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## 2.3 Analysis and Interpretation\n",
    "- After trainig we can finally answer the following questions:\n",
    "\n",
    "---\n",
    "    Q1: Which features significantly impact car prices? Are all\n",
    "features equally important?\n",
    "\n",
    "A1: Using lasso feature selection we find the the most impacting feature on price is `brand`, `enginetype`, etc. as shown below. Also, not all feature have the same impact and this is reflected by the value(weight) associated with each feature. The higher the value the higher the impact.\n",
    "\n",
    "----\n",
    "\n",
    "    Q2: How do a carâ€™s brand and model influence its price\n",
    "prediction?\n",
    "\n",
    "A2: Since both brand and model were one column (`CarName`) after sperating them, the brand shown to be highly affecting the target, while the model is not. This is probably due to the high cardinality of model name, even after attempting to resolve the issue with more appropiate encoding technique such as frequency encoder, it didn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.head(71).groupby(\"original_features\")[\"value\"].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TXez",
   "metadata": {},
   "source": [
    "Q3: Does higher horsepower always result in a higher price?\n",
    "\n",
    "A3: Based on the plot and the high postive correlation, its highly likely that higher horsepower result in higher price. **But** , that is not always the case. Since we can also see from the graph cars with similar horsepower have varying price. That mean other factors influence the final price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "_corr = df[\"horsepower\"].corr(df[\"price\"])\n",
    "print(f\"The correlation between `horsepower`, and `price`: {_corr:.2f} \")\n",
    "sns.scatterplot(x=df[\"horsepower\"], y=df[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {},
   "source": [
    "Q4: How do fuel types and fuel systems affect car pricing?\n",
    "\n",
    "A4: For fueltype, there are more car that uses gas than diesel, and they have a comparable average and variation. **But** , fuel type is fourth most impacting factor in the model. As fuel system, most car uses mpfi and 2bbl systems with varying average and variation, and the impact of fuel system is comparable to fuel type on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='fueltype', y='price', data=df)\n",
    "plt.title('Fuel Type vs. Price')\n",
    "plt.show()\n",
    "df.groupby(\"fueltype\")[\"price\"].agg([\"count\",\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  \n",
    "sns.boxplot(x='fuelsystem', y='price', data=df)\n",
    "plt.title('Fuel System vs. Price')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "df.groupby(\"fuelsystem\")[\"price\"].agg([\"count\",\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAgl",
   "metadata": {},
   "source": [
    "Q5: Is engine size strongly correlated with car price?\n",
    "\n",
    "A5: Based on the plot and the high postive correlation (Pearson correleation coefficient $r=0.87$ ), its seems that `enginesize` and `price are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEll",
   "metadata": {},
   "outputs": [],
   "source": [
    "_corr = df[\"enginesize\"].corr(df[\"price\"])\n",
    "print(f\"The correlation between `enginesize`, and `price`: {_corr:.2f} \")\n",
    "sns.scatterplot(x=df[\"enginesize\"], y=df[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dGlV",
   "metadata": {},
   "source": [
    "Q6: What impact does wheelbase have on car pricing trends?\n",
    "\n",
    "A6: Pearson correlation coefficient is low and postive $r=0.58$ indicating a weaker positve linear relationship with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {},
   "outputs": [],
   "source": [
    "_corr = df[\"wheelbase\"].corr(df[\"price\"])\n",
    "print(f\"The correlation between `wheelbase`, and `price`: {_corr:.2f} \")\n",
    "sns.scatterplot(x=df[\"wheelbase\"], y=df[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgWD",
   "metadata": {},
   "source": [
    "Q7: Does a higher risk rating (positive symboling) increase or decrease the predicted car price?\n",
    "\n",
    "\n",
    "A7: From the boxplot we can see its not monotomically increaseing or decreaseing with price, we can see the average decresasing with increased risk until it reach risk = +1 then start increasing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yOPj",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  \n",
    "sns.boxplot(x='symboling', y='price', data=df)\n",
    "plt.title('Risk vs. Price')\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fwwy",
   "metadata": {},
   "source": [
    "Q8:  Are bore ratio and compression ratio statistically significant in determining car price?\n",
    "\n",
    "\n",
    "A8: Not entirely, while `compressionratio has **_double_** the impact on the model of `boreratio`. Only, compressionration is contributing in the final model between the two, since there other factors that contribute heavly such as brand and engine type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LJZf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance \n",
    "important_features.head(71).groupby(\"original_features\")[\"value\"].sum().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_price_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
